{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4qVLME6q8evMm3PHrFUz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiloufarPadkan/Andrew-NG-Notes/blob/master/deepneuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "soKFAekGmj56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4e0e5d-e3ca-4e64-e016-956d3d8f8c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'W1': array([[ 0.01788628,  0.0043651 ,  0.00096497, -0.01863493, -0.00277388],\n",
            "       [-0.00354759, -0.00082741, -0.00627001, -0.00043818, -0.00477218],\n",
            "       [-0.01313865,  0.00884622,  0.00881318,  0.01709573,  0.00050034],\n",
            "       [-0.00404677, -0.0054536 , -0.01546477,  0.00982367, -0.01101068]]), 'b1': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'W2': array([[-0.01185047, -0.0020565 ,  0.01486148,  0.00236716],\n",
            "       [-0.01023785, -0.00712993,  0.00625245, -0.00160513],\n",
            "       [-0.00768836, -0.00230031,  0.00745056,  0.01976111]]), 'b2': array([[0.],\n",
            "       [0.],\n",
            "       [0.]])}\n",
            "Z = [[0.85391458 1.92375077]]\n",
            "With sigmoid: A = [[0.96890023 0.11013289]]\n",
            "With ReLU: A = [[3.43896131 0.        ]]\n",
            "AL = [[0.78577885 0.87300824]]\n",
            "Length of caches list = 2\n",
            "cost = 0.41493159961539694\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import utils.math as util\n",
        "\n",
        "\n",
        "# initialize and create neural network\n",
        "# it returns a dict with W and b in it\n",
        "\n",
        "def initializer(layer_dims):\n",
        "    np.random.seed(3)\n",
        "    params = {}\n",
        "    for l in range(1, len(layer_dims)):  # from 1 to number of layers\n",
        "        params['W' + str(l)] = np.random.randn(layer_dims[l],\n",
        "                                               layer_dims[l-1]) * 0.01\n",
        "        params['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "        assert(params['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(params['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "params = initializer([5, 4, 3])\n",
        "print(params)\n",
        "\n",
        "# for each layer z(l)=W(l)A(l-1)+b(l)\n",
        "\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "\n",
        "    Z = np.dot(W, A) + b\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "A = np.random.randn(3, 2)\n",
        "W = np.random.randn(1, 3)\n",
        "b = np.random.randn(1, 1)\n",
        "\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        " \n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = util.sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        # This \"activation_cache\" contains \"Z\"\n",
        "        A, activation_cache = util.relu(Z)\n",
        "\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "np.random.seed(2)\n",
        "A_prev = np.random.randn(3, 2)\n",
        "W = np.random.randn(1, 3)\n",
        "b = np.random.randn(1, 1)\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(\n",
        "    A_prev, W, b, activation=\"sigmoid\")\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(\n",
        "    A_prev, W, b, activation=\"relu\")\n",
        "print(\"With ReLU: A = \" + str(A))\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: L_model_forward\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    # number of layers in the neural network\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(\n",
        "            A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    AL, cache = linear_activation_forward(\n",
        "        A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    assert(AL.shape == (1, X.shape[1]))\n",
        "\n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "X = np.random.randn(4, 2)\n",
        "W1 = np.random.randn(3, 4)\n",
        "b1 = np.random.randn(3, 1)\n",
        "W2 = np.random.randn(1, 3)\n",
        "b2 = np.random.randn(1, 1)\n",
        "parameters = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2}\n",
        "AL, caches = L_model_forward(X, parameters)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of caches list = \" + str(len(caches)))\n",
        "\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "  \n",
        "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
        "    cost = np.squeeze(cost)    \n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost\n",
        "\n",
        "Y = np.asarray([[1, 1, 1]])\n",
        "AL = np.array([[.8,.9,0.4]])\n",
        "print(\"cost = \" + str(compute_cost(AL, Y)))\n",
        "\n",
        "\n"
      ]
    }
  ]
}